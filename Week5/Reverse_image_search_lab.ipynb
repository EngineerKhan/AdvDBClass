{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c5a817",
   "metadata": {},
   "source": [
    "# Reverse Image Search — Starter Lab (Beginner‑Friendly)\n",
    "\n",
    "**Goal:** Build a minimal reverse image search pipeline that:\n",
    "- Ingests a small local image folder\n",
    "- Extracts image embeddings (deep features via ResNet50 or a simple color‑histogram fallback)\n",
    "- Performs **top‑K** nearest‑neighbour search (starter: pure NumPy cosine similarity)\n",
    "- Visualizes the results in a neat grid\n",
    "\n",
    "**What *you* will implement later (TODOs in notebook):**\n",
    "- Replace the baseline NumPy search with **FAISS / Annoy / HNSW** OR your own similarity function\n",
    "- Store & fetch embeddings from **MongoDB** (schema stub provided)\n",
    "- (Optional) Try a different embedding model (e.g., CLIP, ViT) and compare quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a088c",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "\n",
    "Put some test images in: `./data/images` (the notebook searches recursively). Any of these formats will work: `jpg, jpeg, png, webp, bmp`.\n",
    "\n",
    "> Tip: 100–500 images is great for testing. More is fine, but the first run will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: installs (uncomment if needed) ---\n",
    "# %pip install --quiet pillow numpy pandas pyarrow matplotlib tqdm torchvision torch\n",
    "# If you plan to do the MongoDB extension:\n",
    "# %pip install --quiet pymongo\n",
    "# For FAISS/Annoy extensions (advanced):\n",
    "# %pip install --quiet faiss-cpu annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220fd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, glob, hashlib, pathlib, io\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep features (optional but recommended)\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    from torchvision import transforms\n",
    "except Exception as e:\n",
    "    torchvision = None\n",
    "    transforms = None\n",
    "    print(\"torchvision not available, deep features will be skipped. Fallback: color histogram. Error:\", e)\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_DIR = \"./data/images\"          # put your images here (recursively)\n",
    "EMBEDDINGS_PATH = \"./embeddings.parquet\"\n",
    "EMBEDDING_MODEL = \"resnet50\"        # options: 'resnet50' or 'hist' (color histogram fallback)\n",
    "IMAGE_EXTS = {\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\"}\n",
    "\n",
    "# Auto-select device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # Apple Silicon\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Images dir: {os.path.abspath(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images(root: str) -> List[str]:\n",
    "    paths = []\n",
    "    root = os.path.expanduser(root)\n",
    "    for ext in IMAGE_EXTS:\n",
    "        paths.extend(glob.glob(os.path.join(root, \"**\", f\"*{ext}\"), recursive=True))\n",
    "    paths = sorted(set(paths))\n",
    "    return paths\n",
    "\n",
    "def sha256_of_file(path: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_image(path: str, max_size: int = 512) -> Image.Image:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    # quick thumbnail to bound max side\n",
    "    img.thumbnail((max_size, max_size))\n",
    "    return img\n",
    "\n",
    "def show_image_grid(paths: List[str], titles: List[str] = None, cols: int = 5, figsize: Tuple[int,int] = (12, 8)):\n",
    "    if not paths:\n",
    "        print(\"No images to display.\")\n",
    "        return\n",
    "    rows = math.ceil(len(paths)/cols)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, p in enumerate(paths):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        try:\n",
    "            img = load_image(p, max_size=512)\n",
    "            plt.imshow(img)\n",
    "        except Exception as e:\n",
    "            plt.text(0.1, 0.5, f\"Failed to load\\n{os.path.basename(p)}\\n{e}\")\n",
    "        plt.axis(\"off\")\n",
    "        if titles and i < len(titles):\n",
    "            plt.title(titles[i], fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f9f14",
   "metadata": {},
   "source": [
    "## Embedding extractors\n",
    "\n",
    "We provide **two** options:\n",
    "\n",
    "1) **ResNet50 (ImageNet)** deep features — better quality, needs `torch` + `torchvision` and will download weights on first run.\n",
    "2) **HSV Color Histogram** (8×8×8 bins) — very simple baseline that works offline. Good as a teaching aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmbeddingResult:\n",
    "    path: str\n",
    "    sha256: str\n",
    "    embedding: np.ndarray\n",
    "\n",
    "def get_resnet50_extractor():\n",
    "    if torchvision is None:\n",
    "        return None, None\n",
    "\n",
    "    # Use torchvision 0.15+ API for weights; fallback if older\n",
    "    weights = None\n",
    "    try:\n",
    "        weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        model = torchvision.models.resnet50(weights=weights)\n",
    "        preprocess = weights.transforms()\n",
    "    except Exception:\n",
    "        # Older API\n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    # Remove the classification head to get a 2048-dim feature\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_one(img: Image.Image) -> np.ndarray:\n",
    "        t = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        feats = model(t)  # [1, 2048]\n",
    "        v = feats.cpu().numpy().astype(\"float32\")[0]\n",
    "        # L2 normalize for cosine\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)\n",
    "        return v\n",
    "\n",
    "    return model, embed_one\n",
    "\n",
    "def embed_histogram(img: Image.Image, bins_per_channel: int = 8) -> np.ndarray:\n",
    "    # HSV histogram (8x8x8 = 512-dim)\n",
    "    hsv = img.convert(\"HSV\")\n",
    "    arr = np.array(hsv)\n",
    "    hist = []\n",
    "    for ch in range(3):\n",
    "        h, _ = np.histogram(arr[..., ch], bins=bins_per_channel, range=(0, 256), density=True)\n",
    "        hist.append(h)\n",
    "    v = np.concatenate(hist).astype(\"float32\")\n",
    "    v = v / (np.linalg.norm(v) + 1e-12)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(paths: List[str], method: str = EMBEDDING_MODEL) -> List[EmbeddingResult]:\n",
    "    results = []\n",
    "    embed_one = None\n",
    "\n",
    "    if method == \"resnet50\":\n",
    "        model, embed_one = get_resnet50_extractor()\n",
    "        if embed_one is None:\n",
    "            print(\"torchvision not available. Falling back to 'hist'.\")\n",
    "            method = \"hist\"\n",
    "\n",
    "    for p in tqdm(paths, desc=f\"Embedding ({method})\"):\n",
    "        try:\n",
    "            img = load_image(p, max_size=1024 if method=='resnet50' else 512)\n",
    "            if method == \"resnet50\":\n",
    "                v = embed_one(img)\n",
    "            else:\n",
    "                v = embed_histogram(img)\n",
    "            results.append(EmbeddingResult(path=p, sha256=sha256_of_file(p), embedding=v))\n",
    "        except Exception as e:\n",
    "            print(\"Failed:\", p, e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99fc25",
   "metadata": {},
   "source": [
    "## Build & persist embeddings\n",
    "\n",
    "This will scan `DATA_DIR`, compute embeddings, and save them to a parquet file so you can load them quickly next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69213631",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list_images(DATA_DIR)\n",
    "print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "embeds = build_embeddings(image_paths, method=EMBEDDING_MODEL)\n",
    "\n",
    "# Pack into a DataFrame\n",
    "if embeds:\n",
    "    dim = embeds[0].embedding.shape[0]\n",
    "else:\n",
    "    dim = 0\n",
    "df = pd.DataFrame({\n",
    "    \"path\": [e.path for e in embeds],\n",
    "    \"sha256\": [e.sha256 for e in embeds],\n",
    "    \"embedding\": [e.embedding.tolist() for e in embeds],\n",
    "    \"dim\": [dim for _ in embeds],\n",
    "})\n",
    "print(df.head())\n",
    "\n",
    "# Save\n",
    "if len(df):\n",
    "    df.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Saved embeddings -> {EMBEDDINGS_PATH} (n={len(df)})\")\n",
    "else:\n",
    "    print(\"No embeddings to save. Did you place images in ./data/images ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f026ad",
   "metadata": {},
   "source": [
    "## Baseline search (NumPy cosine similarity)\n",
    "\n",
    "> **TODO for you later:** Replace this block with FAISS/Annoy/HNSW for speed and try different metrics (cosine/L2/dot). Also, implement batched & cached search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12))\n",
    "\n",
    "def search_topk(query_vec: np.ndarray, matrix: np.ndarray, k: int = 10) -> List[Tuple[int, float]]:\n",
    "    # Returns list of (index, score) sorted by score desc\n",
    "    sims = matrix @ query_vec / (np.linalg.norm(matrix, axis=1) * (np.linalg.norm(query_vec) + 1e-12))\n",
    "    idxs = np.argpartition(-sims, min(k, len(sims)-1))[:k]\n",
    "    idxs = idxs[np.argsort(-sims[idxs])]\n",
    "    return [(int(i), float(sims[i])) for i in idxs]\n",
    "\n",
    "# Materialize the matrix (n, d)\n",
    "if len(df):\n",
    "    M = np.vstack(df[\"embedding\"].apply(lambda x: np.array(x, dtype=\"float32\")).values)  # shape: (n, d)\n",
    "    print(\"Embedding matrix:\", M.shape)\n",
    "else:\n",
    "    M = np.zeros((0,0), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f8199",
   "metadata": {},
   "source": [
    "## Query by example\n",
    "\n",
    "Pick an image path from your dataset (or drag & drop a path into the input). The notebook will:\n",
    "1. Compute its embedding\n",
    "2. Run a top‑K search\n",
    "3. Display the nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a69a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a query image from the dataset:\n",
    "if len(image_paths) >= 1:\n",
    "    QUERY_IMAGE = image_paths[0]\n",
    "else:\n",
    "    QUERY_IMAGE = \"\"  # <- put a path string here like \"./data/images/myphoto.jpg\"\n",
    "\n",
    "print(\"Query image:\", QUERY_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(path: str, method: str = EMBEDDING_MODEL) -> np.ndarray:\n",
    "    if method == \"resnet50\":\n",
    "        model, embed_one = get_resnet50_extractor()\n",
    "        if embed_one is None:\n",
    "            method = \"hist\"\n",
    "    img = load_image(path, max_size=1024 if method=='resnet50' else 512)\n",
    "    if method == \"resnet50\":\n",
    "        _, embed_one = get_resnet50_extractor()\n",
    "        v = embed_one(img)\n",
    "    else:\n",
    "        v = embed_histogram(img)\n",
    "    return v.astype(\"float32\")\n",
    "\n",
    "if QUERY_IMAGE and os.path.exists(QUERY_IMAGE) and len(df):\n",
    "    q = embed_query(QUERY_IMAGE, method=EMBEDDING_MODEL)\n",
    "    topk = search_topk(q, M, k=10)\n",
    "    print(\"Top-K results (index, cosine):\", topk[:5])\n",
    "\n",
    "    result_paths = [df.iloc[i][\"path\"] for i, _ in topk]\n",
    "    result_scores = [f\"{s:.3f}\" for _, s in topk]\n",
    "\n",
    "    print(\"\\nQuery preview:\")\n",
    "    show_image_grid([QUERY_IMAGE], titles=[\"QUERY\"], cols=1, figsize=(4,4))\n",
    "\n",
    "    print(\"Results:\")\n",
    "    show_image_grid(result_paths, titles=result_scores, cols=5, figsize=(12, 8))\n",
    "else:\n",
    "    print(\"Set QUERY_IMAGE to an existing file and ensure embeddings were built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd86f1b",
   "metadata": {},
   "source": [
    "## (Extension) Store & fetch from MongoDB — *you implement this*\n",
    "\n",
    "Below is a **schema stub** and sample code outline (commented). Fill this in your environment when ready.\n",
    "\n",
    "**Document shape** (one per image):\n",
    "```json\n",
    "{\n",
    "  \"sha256\": \"…\",\n",
    "  \"path\": \"./data/images/cats/pic123.jpg\",\n",
    "  \"dim\": 2048,\n",
    "  \"embedding\": [0.01, -0.11, ...]   // float list\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymongo import MongoClient\n",
    "\n",
    "# def get_mongo_collection(uri: str, db_name: str = \"reverse_image_search\", coll_name: str = \"embeddings\"):\n",
    "#     client = MongoClient(uri)\n",
    "#     db = client[db_name]\n",
    "#     return db[coll_name]\n",
    "\n",
    "# def mongo_upsert_embeddings(coll, df: pd.DataFrame, batch_size: int = 500):\n",
    "#     # Upsert by sha256\n",
    "#     ops = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         doc = {\n",
    "#             \"sha256\": row[\"sha256\"],\n",
    "#             \"path\": row[\"path\"],\n",
    "#             \"dim\": int(row[\"dim\"]),\n",
    "#             \"embedding\": row[\"embedding\"],\n",
    "#         }\n",
    "#         ops.append(\n",
    "#             {\n",
    "#                 \"update_one\": {\n",
    "#                     \"filter\": {\"sha256\": row[\"sha256\"]},\n",
    "#                     \"update\": {\"$set\": doc},\n",
    "#                     \"upsert\": True,\n",
    "#                 }\n",
    "#             }\n",
    "#         )\n",
    "#         if len(ops) >= batch_size:\n",
    "#             coll.bulk_write(ops)\n",
    "#             ops = []\n",
    "#     if ops:\n",
    "#         coll.bulk_write(ops)\n",
    "\n",
    "# def mongo_fetch_all_embeddings(coll) -> pd.DataFrame:\n",
    "#     cur = coll.find({}, {\"_id\": 0, \"sha256\": 1, \"path\": 1, \"dim\": 1, \"embedding\": 1})\n",
    "#     rows = list(cur)\n",
    "#     return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595533f",
   "metadata": {},
   "source": [
    "## (Extension) Replace NumPy search with FAISS / Annoy — *you implement this*\n",
    "\n",
    "**Hint:** FAISS cosine search can be done by normalizing vectors and using `IndexFlatIP` (inner product).\n",
    "Annoy supports cosine, angular, Euclidean distances and is super easy to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# def build_faiss_index(matrix: np.ndarray):\n",
    "#     xb = matrix.astype(\"float32\").copy()\n",
    "#     # Normalize for cosine (so IP == cosine)\n",
    "#     faiss.normalize_L2(xb)\n",
    "#     index = faiss.IndexFlatIP(xb.shape[1])\n",
    "#     index.add(xb)\n",
    "#     return index\n",
    "#\n",
    "# def faiss_topk(index, query_vec: np.ndarray, k: int = 10):\n",
    "#     q = query_vec.astype(\"float32\")[None, :].copy()\n",
    "#     faiss.normalize_L2(q)\n",
    "#     sims, idxs = index.search(q, k)\n",
    "#     return list(zip(idxs[0].tolist(), sims[0].tolist()))\n",
    "#\n",
    "# # Example:\n",
    "# # index = build_faiss_index(M)\n",
    "# # faiss_results = faiss_topk(index, q, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aec27f",
   "metadata": {},
   "source": [
    "## Exercises & Discussion\n",
    "\n",
    "1. **Quality check:** Compare results from **ResNet50** vs **color histogram**. Where does histogram fail? Why?\n",
    "2. **Speed:** Time the baseline search (`search_topk`) vs FAISS/Annoy on 10K images (synthetic if needed).\n",
    "3. **MongoDB:** Implement the upsert & fetch functions, rebuild the search from DB rows (not local parquet).\n",
    "4. **Robustness:** Add a **query-by-cropped-region** function (bonus: detect object with `torchvision.ops`).\n",
    "5. **UI:** Export a small **Flask/FastAPI** endpoint that accepts an uploaded image and returns top‑K paths.\n",
    "6. **Evaluation:** If you have class labels, compute **precision@k** for a few queries.\n",
    "7. **CLIP/Vision Transformers (optional):** Swap the encoder and compare."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
